<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>ai-book</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Domains / data</li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.</strong> General</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="general/eda.html"><strong aria-hidden="true">1.1.</strong> EDA</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.2.</strong> Processing</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.3.</strong> Tasks</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">1.3.1.</strong> Anomaly detection</div></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> Computer vision</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.</strong> EDA</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.2.</strong> Processing</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.</strong> Tasks</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.1.</strong> Image classification</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.2.</strong> Semantic segmentation</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.3.</strong> Object detection</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.4.</strong> Instance segmentation</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.5.</strong> Pose estimation</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.6.</strong> Hand pose estimation</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.7.</strong> Reidentification</div></li></ol></li><li class="chapter-item expanded "><a href="computer-vision/models.html"><strong aria-hidden="true">2.4.</strong> Models</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> NLP</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.</strong> Tasks</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.1.</strong> Language modelling</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.2.</strong> Topic modelling</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.3.</strong> Sequence modelling</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.</strong> Tasks</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.1.</strong> Summarisation</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.2.</strong> Machine translation</div></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> Planning</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.</strong> Graph mining</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.</strong> Time series</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.</strong> Audio</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">8.</strong> Multimodal</div></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Concepts</li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.</strong> Knowledge distillation</div></li><li class="spacer"></li><li class="chapter-item expanded "><div><strong aria-hidden="true">10.</strong> Classes of algorithms</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">10.1.</strong> Supervised learning</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">10.2.</strong> Unsupervised learning</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="clustering-algorithms.html"><strong aria-hidden="true">10.2.1.</strong> Clustering algorithms</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">10.2.2.</strong> Dimensionality reduction algorithms</div></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">11.</strong> AutoML</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">12.</strong> Metrics</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">13.</strong> Loss functions</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">14.</strong> Hyperparameter optimisation</div></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">'Learning' types</li><li class="chapter-item expanded "><div><strong aria-hidden="true">15.</strong> Self-supervised learning</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">16.</strong> Semi-supervised learning</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">17.</strong> Transfer learning</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">18.</strong> Deep learning</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">19.</strong> Machine learning</div></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Models / algorithms</li><li class="chapter-item expanded "><a href="k-nn.html"><strong aria-hidden="true">20.</strong> k-NN</a></li><li class="chapter-item expanded "><a href="dbscan.html"><strong aria-hidden="true">21.</strong> DBSCAN</a></li><li class="chapter-item expanded "><a href="linear-models.html"><strong aria-hidden="true">22.</strong> Linear models</a></li><li class="chapter-item expanded "><a href="logistic-regression.html"><strong aria-hidden="true">23.</strong> Logistic regression</a></li><li class="chapter-item expanded "><a href="naive-bayes.html"><strong aria-hidden="true">24.</strong> Naive Bayes</a></li><li class="chapter-item expanded "><a href="k-means.html"><strong aria-hidden="true">25.</strong> k-means</a></li><li class="chapter-item expanded "><a href="hierarchical-clustering.html"><strong aria-hidden="true">26.</strong> Hierachical clustering</a></li><li class="chapter-item expanded "><a href="gmm.html"><strong aria-hidden="true">27.</strong> GMM</a></li><li class="chapter-item expanded "><a href="mean-shift.html"><strong aria-hidden="true">28.</strong> Mean shift</a></li><li class="chapter-item expanded "><a href="decision-tree.html"><strong aria-hidden="true">29.</strong> Decision tree</a></li><li class="chapter-item expanded "><a href="svm.html"><strong aria-hidden="true">30.</strong> SVM</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">31.</strong> Ensemble</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">32.</strong> Neural networks</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">33.</strong> PGM</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">34.</strong> LDA</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">35.</strong> Gaussian process</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">36.</strong> Evolutionary algorithms</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">37.</strong> Association rule mining</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">38.</strong> HMM</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">39.</strong> KDE</div></li><li class="chapter-item expanded "><a href="t-sne.html"><strong aria-hidden="true">40.</strong> t-SNE</a></li><li class="chapter-item expanded "><a href="umap.html"><strong aria-hidden="true">41.</strong> UMAP</a></li><li class="chapter-item expanded "><a href="pca.html"><strong aria-hidden="true">42.</strong> PCA</a></li><li class="chapter-item expanded "><a href="gradient-boosting.html"><strong aria-hidden="true">43.</strong> Gradient boosting</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Representations</li><li class="chapter-item expanded "><div><strong aria-hidden="true">44.</strong> Distance metric</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">45.</strong> Similarity search</div></li><li class="chapter-item expanded "><a href="dimensionality-reduction.html"><strong aria-hidden="true">46.</strong> Dimensionality reduction</a></li><li class="chapter-item expanded "><a href="ann.html"><strong aria-hidden="true">47.</strong> ANN</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Tasks / problems</li><li class="chapter-item expanded "><div><strong aria-hidden="true">48.</strong> Survival analysis</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">49.</strong> Market basket analysis</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">50.</strong> Recommender system</div></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">System design</li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">???</li><li class="chapter-item expanded "><a href="concept-drift.html"><strong aria-hidden="true">51.</strong> Concept drift</a></li><li class="chapter-item expanded "><a href="interpretable-ai.html"><strong aria-hidden="true">52.</strong> Interpretable AI</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ai-book</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="eda"><a class="header" href="#eda">EDA</a></h1>
<p><a href="https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial">https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial</a></p>
<ul>
<li>Missing values
<ul>
<li>Fill using information from correlation</li>
<li>Fill using group information</li>
<li>Fill via knowledge (search)</li>
<li>Fill via heuristics (how a value is obtained in the real world)</li>
</ul>
</li>
<li>Importance of visualisation in the right angle</li>
<li>Distribution of a feature with hue=(train, test)</li>
<li>Remarks</li>
<li>Engineering features based on <em>information gain</em> (against target).</li>
<li>High cardinality indicates that a lot of feature engineering can be done. Convert high cardinality to count. (Is it always okay?)</li>
<li>Create a feature that is your own proxy of the target variable.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="models"><a class="header" href="#models">Models</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Year</th><th>Model</th><th>Modes</th><th>Architecture</th><th>Params</th></tr></thead><tbody>
<tr><td>1998</td><td>LeNet-5</td><td>Image</td><td></td><td>60K</td></tr>
<tr><td>2012</td><td>AlexNet</td><td>Image</td><td></td><td>60M</td></tr>
<tr><td>2014</td><td>VGG-16</td><td>Image</td><td></td><td>138M</td></tr>
<tr><td></td><td>Inception-v1</td><td>Image</td><td></td><td></td></tr>
<tr><td></td><td>GAN</td><td>Image</td><td></td><td></td></tr>
<tr><td>2015</td><td>Inception-v3</td><td>Image</td><td></td><td></td></tr>
<tr><td></td><td>ResNet-50</td><td>Image</td><td></td><td></td></tr>
<tr><td></td><td>U-Net</td><td>Image</td><td>Encoder-decoder</td><td></td></tr>
<tr><td></td><td>DCGAN</td><td>Image</td><td>GAN</td><td></td></tr>
<tr><td>2016</td><td>Xception</td><td>Image</td><td></td><td></td></tr>
<tr><td></td><td>Inception-v4</td><td>Image</td><td></td><td></td></tr>
<tr><td></td><td>Inception-ResNet-V2</td><td>Image</td><td></td><td></td></tr>
<tr><td></td><td>DenseNet</td><td>Image</td><td></td><td></td></tr>
<tr><td></td><td>Show and Tell</td><td>Image + text</td><td></td><td></td></tr>
<tr><td>2017</td><td>ResNeXt-50</td><td>Image</td><td></td><td></td></tr>
<tr><td></td><td>Mask R-CNN</td><td>Image</td><td>Object detection</td><td></td></tr>
<tr><td>2018</td><td>EfficientNet</td><td>Image</td><td></td><td></td></tr>
<tr><td></td><td>BigGAN</td><td></td><td>GAN</td><td></td></tr>
<tr><td>2020</td><td>StyleGAN</td><td>Image</td><td>GAN</td><td></td></tr>
<tr><td>2021</td><td>DALL•E</td><td>Image + text</td><td>Transformer</td><td></td></tr>
<tr><td></td><td>CLIP</td><td>Image + text</td><td>Transformer</td><td></td></tr>
<tr><td></td><td>Latent Diffusion</td><td></td><td></td><td></td></tr>
<tr><td>2022</td><td>Mid Journey</td><td></td><td></td><td></td></tr>
<tr><td></td><td>DALL•E 2</td><td>Image + text</td><td>Transformer</td><td></td></tr>
<tr><td></td><td>Stable Diffusion</td><td>Image + text</td><td></td><td></td></tr>
<tr><td>?</td><td>GPT-3</td><td>Image + text</td><td>Transformer</td><td></td></tr>
<tr><td>?</td><td>GPT</td><td>Image + text</td><td>Transformer</td><td></td></tr>
<tr><td>?</td><td>Fast R-CNN</td><td></td><td>Object detection</td><td></td></tr>
<tr><td>?</td><td>Faster R-CNN</td><td></td><td>Object detection</td><td></td></tr>
<tr><td>?</td><td>CycleGAN</td><td></td><td>GAN</td><td></td></tr>
<tr><td>?</td><td>GauGAN</td><td></td><td>GAN</td><td></td></tr>
<tr><td>?</td><td>YOLO</td><td></td><td>Object detection</td><td></td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="clustering-algorithms"><a class="header" href="#clustering-algorithms">Clustering algorithms</a></h1>
<p>“Meaningful collections / meaningful groups”</p>
<p>“How you choose helps you understand more about them”</p>
<p>Uses:</p>
<ul>
<li>Market segmentation</li>
<li>Medical. imaging</li>
<li>Anomaly detection</li>
<li>Image segmentation</li>
<li>Generalisation (less popular together with more popular videos)</li>
<li>User clustering</li>
</ul>
<p>Why transform data?</p>
<p>In clustering, you calculate the similarity between two examples by combining all the feature data for those examples into a numeric value. <strong>Combing feature data requires that the data have the same scale.</strong></p>
<h2 id="types"><a class="header" href="#types">Types</a></h2>
<ul>
<li>Centroid-based
<ul>
<li>k-means &amp; k-means++ (for choosing seeds)</li>
<li>FAISS (<a href="https://github.com/facebookresearch/faiss">facebookresearch / faiss</a>)</li>
</ul>
</li>
<li>Density-based
<ul>
<li>DBSCAN</li>
<li>OPTICS</li>
</ul>
</li>
<li>Distribution-based</li>
<li>Hierarchical</li>
<li>Unclassified
<ul>
<li>Mean shift</li>
</ul>
</li>
</ul>
<h2 id="metrics"><a class="header" href="#metrics">Metrics</a></h2>
<ul>
<li>Davies-Bouldin index</li>
<li>Silhouette score</li>
<li>Gini coefficient: measure degree of heterogeneity</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="k-nn"><a class="header" href="#k-nn">k-NN</a></h1>
<p><strong>#nonparametric</strong> <strong>#nonprobabilistic</strong></p>
<p>Use a distance metric to calculate the \( k \) closest neighbours. Then, take its mean (or median etc.).</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png" alt="k-NN" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dbscan"><a class="header" href="#dbscan">DBSCAN</a></h1>
<p>Density-based spatial clustering of applications with noise</p>
<p><strong>#nonparametric</strong> <strong>#nonprobabilistic</strong></p>
<p>The DBSCAN algorithm views clusters as areas of high density separated by areas of low density.</p>
<ol>
<li>Choose 2 numbers: max_distance, min_points</li>
<li>Pick a random datapoint. This is cluster A</li>
<li>Criteria for datapoints to be in this cluster:
<ol>
<li>They are within a distance of max_distance</li>
<li>There are more than min_points datapoints</li>
</ol>
</li>
<li>Once run out of points, find another datapoint that has no cluster. This is Cluster B.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linear-models"><a class="header" href="#linear-models">Linear models</a></h1>
<p>Uses OLS. See page for assumptions. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logistic-regression"><a class="header" href="#logistic-regression">Logistic regression</a></h1>
<p><strong>#parametric</strong> <strong>#probabilistic</strong></p>
<p>A model that predicts the probability using the logistic function $$\frac{1}{1+\exp(-x)}$$</p>
<p>Model:</p>
<p>$$
\hat{p}=\frac{1}{1+\exp(-\sum_i w_i x_i)} \in[0,1]
$$</p>
<p>A regression model whose output (termed as logits) is mapped from a real number to $(0,1)$ through the use of logistic function, which is the inverse of log odds. Log of odds is a measure of odds, which is a ratio (of probabilities).</p>
<p>To get the loss function, notice that each sample can be considered as a Bernoulli trial hence the model follows a Bernoulli distribution. Our objective is to maximise the likelihood, which is equivalent to minimising the negative of maximum likelihood.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="naive-bayes"><a class="header" href="#naive-bayes">Naive Bayes</a></h1>
<p><strong>#probabilistic</strong></p>
<p>Assumes strong independence assumption between features, where each feature x is a particular distribution (usually Gaussian), so need to get the mean and std dev.</p>
<p><a href="https://remykarem.github.io/blog/naive-bayes.html">https://remykarem.github.io/blog/naive-bayes.html</a></p>
<p>Inference time:</p>
<p>$$
\mathbf{P}(\text{class} | x) = \frac{\mathbf{P}(\text{class}) × \mathbf{P}(x | \text{class})}{\mathbf{P}(\text{data})}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="k-means"><a class="header" href="#k-means">k-means</a></h1>
<p><strong>#nonparametric</strong></p>
<p>Assign k centroids randomly</p>
<p>Get each data point to see which centroid it’s closest to, and assign a class</p>
<p>Calculate centre of points for each group</p>
<p>Reassign the k centroids</p>
<p>Use silhouette heuristic or the elbow method to determine a reasonable value for k</p>
<p><img src="./k-means.png" alt="k-means" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hierachical-clustering"><a class="header" href="#hierachical-clustering">Hierachical clustering</a></h1>
<ul>
<li>agglomerative (slowly accumulate)</li>
<li>divisive (slowly split)</li>
</ul>
<p>This is achieved by use of a metric and a linkage criterion.</p>
<p>Examples of linkage criteria:</p>
<ul>
<li>Single linkage (minimum)</li>
<li>Complete linkage (maximum)</li>
<li>Average linkage (average between pairs)</li>
<li>Ward</li>
<li>Centroid</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gmm"><a class="header" href="#gmm">GMM</a></h1>
<p>Gaussian Mixture Model</p>
<p><strong>#parametric</strong> <strong>#probabilistic</strong></p>
<p>A probabilistic model that assumes all the data points are generated from a mixture of Gaussian distributions with unknown parameters</p>
<p>Optimiser: EM algorithm</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mean-shift"><a class="header" href="#mean-shift">Mean shift</a></h1>
<p><strong>#nonparametric</strong></p>
<p>Popular in computer vision</p>
<p>Builds on KDE</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="decision-tree"><a class="header" href="#decision-tree">Decision tree</a></h1>
<p><strong>#nonparametric</strong> <strong>#nonprobabilistic</strong></p>
<p>Keep splitting, split based on highest Gini Gain or Information Gain.</p>
<blockquote>
<p>💡 Gini impurity is a way of measuring if a split is &quot;good&quot;. Calculate Gini impurities (1) before split, (2) left branch and (3) right branch. Then weight the impurities of each branch (2) and (3). Then calculate how much impurity is removed by taking (1) minus the weighted (2) and (3).</p>
</blockquote>
<p>Try every split:</p>
<ul>
<li>Every feature</li>
<li>Every possible threshold (optimisable)</li>
</ul>
<p>Stop when:</p>
<ul>
<li>All Gini gains are the same</li>
<li>No Gini gain</li>
</ul>
<p>Algorithms:</p>
<ul>
<li>ID3 (multiway)
<ul>
<li>Brute force</li>
</ul>
</li>
<li>C4.5 (binary)
<ul>
<li>Brute force</li>
</ul>
</li>
<li>CART
<ul>
<li>Construct binary trees from using features and thresholds that yield highest information gain</li>
</ul>
</li>
</ul>
<blockquote>
<p>👉 Tree-based algorithms don’t require scaling!</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="svm"><a class="header" href="#svm">SVM</a></h1>
<p>Support Vector Machine</p>
<p>Find a separate hyperplane (&quot;decision boundary&quot;) that has the highest margin. The datapoints which are at the margin are called support vectors.</p>
<p>“Large-margin classifier”</p>
<ul>
<li>Linear separable</li>
<li>Nonlinear separable
<ul>
<li><strong>Soft Margin</strong>
Two types of misclassifications are tolerated by SVM under soft margin:
<ul>
<li>The dot is on the wrong side of the decision boundary but on the correct side/ on the margin (shown in left)</li>
<li>The dot is on the wrong side of the decision boundary and on the wrong side of the margin (shown in right)</li>
</ul>
</li>
<li><strong>Kernel Tricks</strong>
<ul>
<li>What Kernel Trick does is it utilises existing features, applies some transformations, and creates new features.</li>
<li>Think of the polynomial kernel as a transformer/processor to generate new features by applying the polynomial combination of all the existing features.</li>
<li>Think of the Radial Basis Function kernel as a transformer/processor to generate new features by measuring the distance between all other dots to a specific dot/dots — centres.</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="t-sne"><a class="header" href="#t-sne">t-SNE</a></h1>
<p><strong>#nonparametric</strong></p>
<p>t-Distributed Stochastic Neighbour Embedding (2008)</p>
<ul>
<li>Unsupervised non-linear technique</li>
<li>Things that are close remain close (?)</li>
<li>preserving only small pairwise distances or local similarities</li>
<li>Perplexity ~ a guess about the number of close neighbours each point has. 5-50</li>
<li>Creates 2D maps from multidimensional data</li>
<li>More computationally expensive</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="umap"><a class="header" href="#umap">UMAP</a></h1>
<p><strong>#nonparametric</strong> <strong>#nonprobabilistic</strong></p>
<p>Uniform Manifold Approximation and Projection</p>
<ol>
<li>The data is uniformly distributed on a Riemannian manifold;</li>
<li>The Riemannian metric is locally constant (or can be approximated as such);</li>
<li>The manifold is locally connected.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pca"><a class="header" href="#pca">PCA</a></h1>
<p>Principal Component Analysis</p>
<p><strong>#parametric</strong> <strong>#nonprobabilistic</strong></p>
<p><a href="https://towardsdatascience.com/visualizing-principal-component-analysis-with-matrix-transforms-d17dabc8230e?gi=2dbc0f4e5157">https://towardsdatascience.com/visualizing-principal-component-analysis-with-matrix-transforms-d17dabc8230e?gi=2dbc0f4e5157</a></p>
<p>Premise: the useful information is the underlying axes of data that gives you most variance</p>
<ul>
<li>Linear dimension reduction technique</li>
<li>Things that are different end up very far apart</li>
<li>Maximises variance and preserves large pairwise distances</li>
<li>Computationally cheap</li>
<li>“Redefine new axes”</li>
</ul>
<p><strong>Principal components</strong></p>
<p>They are the</p>
<ul>
<li>Underlying “axes” of data</li>
<li>= “directions” (= eigenvectors) where the data is most spread out, i.e have the most variance (variance = eigenvalues)
<ul>
<li>No. of eigenvalues/eigenvectors equals to no. of dimensions of data</li>
<li>The eigenvectors are perpendicular (= orthogonal) to each other</li>
</ul>
</li>
</ul>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Generate data
angle = np.radians(30.)
x1 = np.random.normal(scale=8, size=100)
x2 = x1*np.tan(angle) + np.random.normal(scale=3, size=100)
# x2 = np.random.normal(scale=3, size=100)
x1 = x1-np.mean(x1)
x2 = x2-np.mean(x2)
data = np.array([x1, x2])

# PCA
pca = PCA()
pca.fit(data.T)

# Matrix transformation
# angle = np.radians(30. - 360.) # same
angle_derotate = -angle
derotate = np.array([[np.cos(angle_derotate), -np.sin(angle_derotate)],
                     [np.sin(angle_derotate), np.cos(angle_derotate)]])
derotated_data = np.matmul(derotate, data)

rotate = np.array([[np.cos(angle_derotate), np.sin(angle_derotate)],
                     [-np.sin(angle_derotate), np.cos(angle_derotate)]])
rotated_data = np.matmul(data.T, rotate)

# Get explained variances
print(&quot;Variance in X: {:0.1f}&quot;.format(np.var(derotated_data[0])))
print(&quot;Variance in Y: {:0.1f}&quot;.format(np.var(derotated_data[1])))
print(pca.explained_variance_)

# Get the principal components
print(derotate)
print(rotate)
print(pca.components_.T)

# Covariance
# print(np.cov(data))
# print(np.matmul(np.matmul(derotate, data).T, rotate)) ????
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-boosting"><a class="header" href="#gradient-boosting">Gradient boosting</a></h1>
<p><img src="./gradient-boosting.png" alt="Gradient boosting" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dimensionality-reduction"><a class="header" href="#dimensionality-reduction">Dimensionality reduction</a></h1>
<p>to create embeddings or projection of high-dimensional vector to 3D</p>
<p>(unsupervised learning)</p>
<p>Dimensionality reduction. Reducing the dimension helps us to visualise better</p>
<p>PCA - keep global distance</p>
<p>t-SNE - focus on local distance</p>
<p>UMAP - focus on local distance, but keep more global distance</p>
<p>Random projection</p>
<p>Runtime: tSNE&gt;UMAP≥PCA</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ann"><a class="header" href="#ann">ANN</a></h1>
<p>Approximate nearest neighbours</p>
<p>Fast computation of nearest neighbours is an active area of research. The most naive implementation i s the brute-force computation of distances between all pairs.</p>
<p><a href="http://ann-benchmarks.com/">http://ann-benchmarks.com</a></p>
<p>Fast approximate distance computations for vector similarity search</p>
<ul>
<li><a href="https://github.com/google-research/google-research/tree/master/scann">ScaNN</a></li>
<li>NGT-ONNG</li>
<li>hnswlib</li>
<li><a href="https://github.com/facebookresearch/faiss">FAISS</a></li>
<li><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwik7vrY9oLrAhVLAXIKHfz3AC4QFjAAegQIAxAC&amp;url=https%3A%2F%2Fgithub.com%2Fspotify%2Fannoy&amp;usg=AOvVaw1EAessPsvKY5ftaBM6VmqC">ANNOY</a></li>
<li>scikit-learn: K-D tree, Ball Tree</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="concept-drift"><a class="header" href="#concept-drift">Concept drift</a></h1>
<p>data drift, label schema change, model drift</p>
<p><a href="https://twitter.com/chipro/status/1313921889061015557?s=20">https://twitter.com/chipro/status/1313921889061015557?s=20</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="interpretable-ai"><a class="header" href="#interpretable-ai">Interpretable AI</a></h1>
<p>Explainable AI / Bias in AI</p>
<p>Resources</p>
<p><a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a></p>
<p><a href="https://github.com/jphall663/awesome-machine-learning-interpretability">https://github.com/jphall663/awesome-machine-learning-interpretability</a></p>
<p><a href="https://microscope.openai.com/models">https://microscope.openai.com/models</a></p>
<p><strong>Explainability</strong> refers to the understanding, in simple terms, of how exactly a model works under the hood.</p>
<p><strong>Interpretability</strong> refers to the ability of observing the effect that changes in the (i) input or (ii) parameters will have on predicted outputs</p>
<p>What is?</p>
<ul>
<li>Extracting insights to understand
<ul>
<li>Why did the model make such a prediction?</li>
<li>How does each feature affect a prediction? What if I did this?</li>
</ul>
</li>
</ul>
<p>For?</p>
<ul>
<li>Trust</li>
<li>Debugging</li>
<li>Future data collection</li>
<li>Feature engineering</li>
<li>Inform human decision-making</li>
</ul>
<p><strong>General</strong></p>
<ul>
<li>Model cards</li>
<li>explainx.ai</li>
</ul>
<h1 id="tabular"><a class="header" href="#tabular"><strong>Tabular</strong></a></h1>
<h2 id="feature-importance"><a class="header" href="#feature-importance"><strong>Feature importance</strong></a></h2>
<ul>
<li>Tree-based models</li>
<li>LIME</li>
<li>Permutation importance: shuffle the data within a column</li>
<li>Partial dependence plots: keep changing value for one variable</li>
<li>SHAP values (Shapley additive explanations): compare a value with baseline value</li>
<li>Integrated Gradients</li>
<li>DeepLift</li>
<li>Model-Agnostic Linear Competitors (MALC)</li>
<li><a href="https://github.com/pytorch/captum">Captum</a></li>
</ul>
<h2 id="constraining-nonlinear-models"><a class="header" href="#constraining-nonlinear-models"><strong>Constraining nonlinear models</strong></a></h2>
<ul>
<li>Contextual Decomposition Explanation Penalization (CDEP) add a term to the loss function so models can learn how to produce good explanations</li>
<li>MonoNet</li>
</ul>
<h2 id="interpretable-models"><a class="header" href="#interpretable-models"><strong>Interpretable models</strong></a></h2>
<ul>
<li>Decision trees: 
Information gain &amp; gini impurity. Splits that have the most impact on a prediction are kept closer to the root of the tree</li>
<li>Linear models
Implicitly interpretable, since they can naturally weight the influence of the weights. Perturbing the inputs or learned parameters has a predetermined effect on the outputs.</li>
<li>Logistic regression</li>
</ul>
<h1 id="image"><a class="header" href="#image"><strong>Image</strong></a></h1>
<p>Using feature visualisation and activation atlases</p>
<ul>
<li>
<p>Grad-CAM (Class activation map)</p>
<p>Grad-CAM works by (1) finding the final convolutional layer in the network and then (2) examining the gradient information flowing into that layer. Then multiply the signals with the outputs from (1).</p>
</li>
<li>
<p><a href="https://github.com/marcotcr/lime">LIME</a></p>
</li>
<li>
<p>Summit</p>
</li>
</ul>
<h1 id="text"><a class="header" href="#text"><strong>Text</strong></a></h1>
<ul>
<li>exBERT</li>
<li><a href="https://github.com/jalammar/ecco?utm_source=Deep+Learning+Weekly&amp;utm_campaign=6f762da6bd-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&amp;utm_medium=email&amp;utm_term=0_384567b42d-6f762da6bd-72969297">Ecco</a></li>
<li>Language Interpretability Tool by Google</li>
</ul>
<p>Libraries</p>
<ul>
<li>ELI5</li>
<li>pdpbox</li>
<li>shap</li>
<li>Uber’s manifold</li>
<li>What-If</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
