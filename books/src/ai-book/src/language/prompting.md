# Prompting

Pre-trained language models can solve tasks using zero-shot or few-shot prompting as in the paper [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165).

This technique requires _prompt engineering_ â€” designing tasks that look like the data the model has seen during training.

- Chain-of-Thought [^cot]
- Retrieval-Augemented QA
- Self-Ask - ([Press et al 2022](https://ofir.io/self-ask.pdf))

---

[^cot]: [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
