# Models

Keywords: MLLM (Multimodal Large Language Model)

| Year | Model              | Modes                                 | Architecture | Params |
| ---- | ------------------ | ------------------------------------- | ------------ | ------ |
| 2023 | PaLM-E [^palme]    | Vison + language + planning (robotic) |              | 562B   |
|      | USM [^usm]         | Audio                                    |              |        |
|      | Kosmos-1[^kosmos1] | Vision + language                                       |              |        |
|      | Flamingo           |                                       |              |        |
|      | PaLI               |                                       |              |        |
| 2022 | DALL•E 2           | Vision + text                         | Transformer  |        |
| 2022 | Stable Diffusion   | Vision + text                         |              |        |
| 2021 | DALL•E             | Vision + language                     | Transformer  |        |
|      | CLIP [^clip]       | Vision + language                     | Transformer  |        |
|      | Stable Diffusion   | Vision + language                     |              |        |
|      | DALL•E             | Vision + text                         | Transformer  |        |
| ?    | GPT-3              | Vision + language                     | Transformer  |        |
| ?    | GPT                | Vision + language                     | Transformer  |        |
| 2016 | Show and Tell      | Vision + text                         |

[^palme]: [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io/)

[^usm]: [Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages](https://arxiv.org/abs/2303.01037)

[^kosmos1]: [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/230214045)

[^clip]: [CLIP](https://openai.com/blog/clip/)
