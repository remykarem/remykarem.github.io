<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Planning in a stochastic environment - AI Planning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="terminologies.html"><strong aria-hidden="true">1.</strong> Terminologies</a></li><li class="chapter-item expanded "><a href="representations.html"><strong aria-hidden="true">2.</strong> Representations</a></li><li class="chapter-item expanded "><a href="knowledge-base.html"><strong aria-hidden="true">3.</strong> Knowledge base</a></li><li class="chapter-item expanded "><a href="problem.html"><strong aria-hidden="true">4.</strong> Problem</a></li><li class="chapter-item expanded "><a href="resources.html"><strong aria-hidden="true">5.</strong> Resources</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="simple-planning.html"><strong aria-hidden="true">6.</strong> Simple planning</a></li><li class="chapter-item expanded "><a href="classical-planning.html"><strong aria-hidden="true">7.</strong> Classical planning</a></li><li class="chapter-item expanded "><a href="planning-in-a-stochastic-environment.html" class="active"><strong aria-hidden="true">8.</strong> Planning in a stochastic environment</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="value-iteration.html"><strong aria-hidden="true">8.1.</strong> Value iteration</a></li><li class="chapter-item expanded "><a href="policy-iteration.html"><strong aria-hidden="true">8.2.</strong> Policy iteration</a></li><li class="chapter-item expanded "><a href="gpi.html"><strong aria-hidden="true">8.3.</strong> GPI</a></li><li class="chapter-item expanded "><a href="mcts.html"><strong aria-hidden="true">8.4.</strong> Online search: MCTS</a></li></ol></li><li class="chapter-item expanded "><a href="learning-to-plan-in-a-stochastic-environment.html"><strong aria-hidden="true">9.</strong> Learning to plan in a stochastic environment</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="learning-the-q-function.html"><strong aria-hidden="true">9.1.</strong> Learning the Q-function</a></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">9.1.1.</strong> MC learning</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="direct-utility-estimation.html"><strong aria-hidden="true">9.1.1.1.</strong> Direct utility estimation</a></li><li class="chapter-item expanded "><a href="mc-control-with-gpi.html"><strong aria-hidden="true">9.1.1.2.</strong> MC control with GPI</a></li><li class="chapter-item expanded "><a href="glie-epsilon-greedy-mc-control.html"><strong aria-hidden="true">9.1.1.3.</strong> GLIE epsilon-greedy MC control</a></li></ol></li><li class="chapter-item expanded "><a href="td-learning.html"><strong aria-hidden="true">9.1.2.</strong> TD learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="sarsa.html"><strong aria-hidden="true">9.1.2.1.</strong> SARSA</a></li><li class="chapter-item expanded "><a href="q-learning.html"><strong aria-hidden="true">9.1.2.2.</strong> Q-learning</a></li><li class="chapter-item expanded "><a href="deep-q-learning.html"><strong aria-hidden="true">9.1.2.3.</strong> Deep Q-learning</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="learning-the-policy.html"><strong aria-hidden="true">9.2.</strong> Learning the policy</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="policy-gradient.html"><strong aria-hidden="true">9.2.1.</strong> Policy gradient</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="reinforce.html"><strong aria-hidden="true">9.2.1.1.</strong> REINFORCE</a></li><li class="chapter-item expanded "><a href="reinforce-with-baseline.html"><strong aria-hidden="true">9.2.1.2.</strong> REINFORCE with baseline</a></li><li class="chapter-item expanded "><a href="actor-critic.html"><strong aria-hidden="true">9.2.1.3.</strong> Actor critic</a></li><li class="chapter-item expanded "><a href="trpo.html"><strong aria-hidden="true">9.2.1.4.</strong> TRPO</a></li><li class="chapter-item expanded "><a href="ppo.html"><strong aria-hidden="true">9.2.1.5.</strong> PPO</a></li><li class="chapter-item expanded "><a href="imitation-learning.html"><strong aria-hidden="true">9.2.1.6.</strong> Imitation learning</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="learning-the-transition-model.html"><strong aria-hidden="true">9.3.</strong> Learning the transition model</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="adp.html"><strong aria-hidden="true">9.3.1.</strong> ADP</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="planning-in-a-stochastic-environment-with-partial-observability.html"><strong aria-hidden="true">10.</strong> Planning in a stochastic environment with partial observability</a></li><li class="chapter-item expanded "><a href="planning-in-a-multi-agent-environment.html"><strong aria-hidden="true">11.</strong> Planning in a multi-agent environment</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="single-move-games.html"><strong aria-hidden="true">11.1.</strong> Single-move / simultaneous games</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="2-player-non-zero-sum-games.html"><strong aria-hidden="true">11.1.1.</strong> 2-player non-zero-sum games</a></li><li class="chapter-item expanded "><a href="2-player-zero-sum-games.html"><strong aria-hidden="true">11.1.2.</strong> 2-player zero-sum games</a></li><li class="chapter-item expanded "><a href="other-games.html"><strong aria-hidden="true">11.1.3.</strong> Other games</a></li></ol></li><li class="chapter-item expanded "><a href="sequential-games.html"><strong aria-hidden="true">11.2.</strong> Sequential games</a></li><li class="chapter-item expanded "><a href="repeated-games.html"><strong aria-hidden="true">11.3.</strong> Repeated games</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><li class="part-title">Reinforcement learning</li><li class="chapter-item expanded "><a href="multi-armed-bandit-problem.html"><strong aria-hidden="true">12.</strong> Multi-armed bandit problem</a></li><li class="chapter-item expanded "><a href="characteristics-of-learning.html"><strong aria-hidden="true">13.</strong> Characteristics of learning</a></li><li class="chapter-item expanded "><a href="on-vs-off-policy-learning.html"><strong aria-hidden="true">14.</strong> On-policy vs. off-policy learning</a></li><li class="chapter-item expanded "><a href="exploration-algorithms.html"><strong aria-hidden="true">15.</strong> Exploration algorithms</a></li><li class="chapter-item expanded "><a href="deadly-triad.html"><strong aria-hidden="true">16.</strong> Deadly triad</a></li><li class="chapter-item expanded "><a href="function-approximation.html"><strong aria-hidden="true">17.</strong> Function approximation</a></li><li class="chapter-item expanded "><a href="applications.html"><strong aria-hidden="true">18.</strong> Applications</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Appendix</li><li class="chapter-item expanded "><a href="decision-theory.html"><strong aria-hidden="true">19.</strong> Decision theory</a></li><li class="chapter-item expanded "><a href="principal-of-optimality.html"><strong aria-hidden="true">20.</strong> Principal of optimality</a></li><li class="chapter-item expanded "><a href="counting.html"><strong aria-hidden="true">21.</strong> Counting</a></li><li class="chapter-item expanded "><a href="mdp.html"><strong aria-hidden="true">22.</strong> MDP</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">AI Planning</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="planning-in-a-stochastic-environment"><a class="header" href="#planning-in-a-stochastic-environment">Planning in a stochastic environment</a></h1>
<p>To model decision making in situations where outcomes are partly random and partly under the control of a decision maker, we use a mathematical model called <strong>Markov decision process.</strong></p>
<p>Markov Decision Process = &quot;if i see this state, i can forget about the history&quot;</p>
<p>= (states, actions, rewards, transitions)</p>
<p>Settings</p>
<ul>
<li>Horizon (finite or infinite)</li>
<li>Discounted or not discounted</li>
</ul>
<p>Markov decision process: every decision point does not require you to know about the previous history</p>
<p>How to plan in a stochastic environment? We want a policy that tells us what action to take given a state. To get this policy, we need to run value iteration or policy iteration. Sometimes we don't need to have the entire policy possibly because there are too many states. In this case, we perform online search (finding the best action at that state).</p>
<hr />
<ul>
<li><a href="#state">State</a></li>
<li><a href="#action">Action</a></li>
<li><a href="#transition-matrix">Transition matrix</a></li>
<li><a href="#reward-and-reward-function">Reward and reward function</a></li>
<li><a href="#policy">Policy</a></li>
<li><a href="#utility-of-a-state">Utility of a state</a></li>
<li><a href="#value-function">Value function</a></li>
<li><a href="#q-function">Q-function</a></li>
<li><a href="#sum-of-rewards">Sum of rewards</a></li>
<li><a href="#trajectory">Trajectory</a></li>
<li><a href="#advantage-function">Advantage function</a></li>
</ul>
<hr />
<h2 id="state"><a class="header" href="#state">State</a></h2>
<p>$$
s
$$</p>
<h2 id="action"><a class="header" href="#action">Action</a></h2>
<p>$$
a
$$</p>
<h2 id="transition-matrix"><a class="header" href="#transition-matrix">Transition matrix</a></h2>
<blockquote>
<p>üí° If I take action \( a \) at state \( s \), what is the probability that I end up at state \( s' \)?</p>
</blockquote>
<blockquote>
<p>‚ö†Ô∏è Note that action has already been determined.</p>
</blockquote>
<p>$$
T(s, a) \rightarrow s'
$$</p>
<p>$$
P(s'|s,a) \rightarrow \mathbb{R}
$$</p>
<blockquote>
<p>‚ö†Ô∏è Transitions must be Markovian</p>
</blockquote>
<h2 id="reward-and-reward-function"><a class="header" href="#reward-and-reward-function">Reward and reward function</a></h2>
<p>A function (&quot;<strong>reward function</strong>&quot;) that maps a state \( s \) to a value (&quot;<strong>reward</strong>&quot;).</p>
<p>$$
R(s) \rightarrow \mathbb{R}
$$</p>
<p>Reward functions can also depend on both the state and the action:</p>
<p>$$
R(s,a) \rightarrow \mathbb{R}
$$</p>
<h2 id="policy"><a class="header" href="#policy">Policy</a></h2>
<p>A function (&quot;<strong>policy</strong>&quot;) that maps a state to an action. It answers the question &quot;what action should I take at a particular state?&quot;</p>
<p>$$
\pi(s) \rightarrow a
$$</p>
<p>Utility of a sequence of states is the sum of (discounted) rewards</p>
<p>$$
U([s_0, s_1, ...]) \rightarrow \mathbb{R}
$$</p>
<h2 id="utility-of-a-state"><a class="header" href="#utility-of-a-state">Utility of a state</a></h2>
<p>The utility of a state is a measure of how ‚Äúuseful‚Äù a state is. To be used later to compare between utilities of other states.</p>
<p>$$
U(s) \rightarrow \mathbb{R}
$$</p>
<p>$$
\begin{aligned}
U(s) 
&amp;= \mathbb{E} [\text{(discounted) sum of all rewards}] \\
&amp;= \mathbb{E} [R(S_0) + \gamma R(S_1) + ... ] \\
&amp;= \mathbb{E} \Big[ \sum_t \gamma^t R(S_t) \Big]
\end{aligned}
$$</p>
<p>= utility of a sequence of states as taken by some policy \( \pi \)</p>
<p>= rewards from current state + rewards from future states</p>
<h2 id="value-function"><a class="header" href="#value-function">Value function</a></h2>
<p>The value function is the utility of a state under an <em>optimal</em> policy. It is the Bellman equation itself.</p>
<blockquote>
<p>üí° The value function of a state is the sum of (1) the current reward and (2) the expected (or weighted) value from taking the ‚Äòbest‚Äô action.</p>
</blockquote>
<p>$$
\begin{aligned}
V(s) 
&amp;= R(s) + \gamma \max_{a \in A} \sum_{s'} P(s'|s,a)V(s') \
\end{aligned}
$$</p>
<p>For MDPs where the reward function depends on both state and action,</p>
<p>$$
\begin{aligned}
V(s) 
&amp;= \max_{a\in A} \Big(R(s,a) + \gamma \mathbb{E}[U(s')] \Big) \\
&amp;= \max_{a\in A} \Big(R(s,a) + \gamma \sum_{s'} P(s'|s,a)V(s') \Big)
\end{aligned}
$$</p>
<p>From the value function \( V \), we can derive the optimal policy \( \pi^* \)</p>
<p>$$
\pi^*(s) = \arg \max_a \sum_{s'} P(s'|s,a)  V(s')
$$</p>
<blockquote>
<p>üí° Suppose you are in a state \( s \). For every action in this state, compute its weighted value (based on the transition probabilities). Then find the action that returns the highest expected value.</p>
</blockquote>
<h2 id="q-function"><a class="header" href="#q-function">Q-function</a></h2>
<p>Like the value function, the Q-function is the utility of taking an action at a given state under an <em>optimal</em> policy.</p>
<blockquote>
<p>üí° (what's the expected reward given that you start from a certain state \( s \) and take a certain action \( a \))</p>
</blockquote>
<p>$$
Q(s,a) = R(s) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q(s', a')
$$</p>
<p>The higher the Q-value, the more desirable is the action at that state.</p>
<p>From the Q-function, we can derive the optimal policy \( \pi^* \):</p>
<p>$$
\pi^*(s) = \arg \max_a Q(s,a)
$$</p>
<h2 id="sum-of-rewards"><a class="header" href="#sum-of-rewards">Sum of rewards</a></h2>
<p>The sum of rewards for an episode that starts with state \( s \).</p>
<p>$$
G(s)
$$</p>
<p>Two notations exist:</p>
<ol>
<li>\( G_k \) indicates the cumulative rewards for the \( k \)th episode.</li>
<li>\( G_{t:t+n} = R_t + R_{t+1} + ... +  R_{t+n-1} + \hat{U}(S_{t+n}) \) (discounts are omitted for readability)</li>
</ol>
<h2 id="trajectory"><a class="header" href="#trajectory">Trajectory</a></h2>
<p>A sequence of states that have been undertaken is called a &quot;<strong>trajectory</strong>&quot; or an &quot;<strong>episode</strong>&quot;.</p>
<p>$$
\tau
$$</p>
<h2 id="advantage-function"><a class="header" href="#advantage-function">Advantage function</a></h2>
<p>$$
A(s,a) = Q(s,a) - V(s)
$$</p>
<blockquote>
<p>üí° How much is a certain action good or bad given a certain state. how relatively happy you are taking the action a (given that you're in state s) = what's so good about taking action a = what's the advantage of taking action a. I used 'relatively' because Q(s,a) has a similar definition but without the relative.</p>
</blockquote>
<p>Calculating no. of states:</p>
<ul>
<li>
<p>TSP</p>
<p>\( N \) cities. A salesman must visit every city in a graph. Cannot visit any city twice. </p>
<p>State: visited 0, at city 4</p>
<p>State: visited 1, at city 2</p>
<p>For every time he is at a city, there are \( n \le N \) cities visited.</p>
<p>There are \( 2^N \) combinations where he can be.</p>
<p>$$O(N2^N)$$</p>
</li>
<li>
<p>Inventory control</p>
<p>Company has space to store \( N \).</p>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="classical-planning.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="value-iteration.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="classical-planning.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="value-iteration.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
