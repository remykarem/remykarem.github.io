# Fine-tuning

* Task-specific

* Instruction-tuning / instruction fine-tuning [^instructgpt]

    Creating a dataset of instructions from scratch to fine-tune the model would take a considerable amount of resources. Therefore, we instead make use of templates to transform existing datasets into an instructional format.[^flan]

    Instruction-tuned language models demonstrate a remarkable ability to generalise zero-shot to new tasks.

    Example: FLAN â€” Fine-tuned LAnguage Net

* Though process fine-tuning [^thoughtprocess]

    Decompose the problem into steps

* PaL (Program-aided Language) [^pal]

    Decompose the problem into *programmatic* steps

* RLHF (Reinforcement Learning from Human Feedback)

* RLAIF (RL from AI Feedback) [Paper](https://arxiv.org/abs/2212.08073)

---

[^instructgpt]: [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)

[^flan]: [Flan](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)

[^thoughtprocess]: [Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge](https://arxiv.org/abs/2006.06609)

[^pal]: [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)
