# Hyperparameter optimisation

1. HyperBand and ASHA (Asynchronous Successive ZHalving Algorithm): random + optimise model
2. Bayesian Optimisation
    - optimal, but public packages arenâ€™t great
3. Random forest
4. 1cycle policy (learning rate for deep learning tasks)
5. Population-based training

## Grid search

Inefficient

## Random search

Easy and near optimal

Libraries

- RayTune
- Katib
- Hyperas
- SIGOPT
- Sweeps
- Keras Tuner
