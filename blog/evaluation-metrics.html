<!DOCTYPE html>

<!-- KaTeX requires the use of the HTML5 doctype. Without it, KaTeX may not render properly -->
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" description="A cheat sheet of the common evaluation metrics with code" content="width=device-width, initial-scale=1">
    <title>Cheat Sheet: Evaluation Metrics</title>

    <!-- Load KaTeX CSS first -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css"
        integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">

    <!-- Load Distill Pub template -->
    <script src="js/distill_template_modified.v2.js"></script>

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js"
        integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O"
        crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
        integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <script src="https://code.jquery.com/jquery-3.4.1.slim.js"
        integrity="sha256-BTlTdQO9/fascB1drekrDVkaKd9PkwBymMlHOiG+qLI=" crossorigin="anonymous"></script>

    <script type="text/front-matter">
            title: "Evaluation Metrics WIP"
            description: "Description of the post"
            authors:
            - Raimi Karim: http://raibosome.github.io
            affiliations:
            - AI Singapore: https://aisingapore.com
              </script>
</head>

<body>
    <d-title>
        <h1>Evaluation metrics for classification</h1>
        <p>A cheat sheet of the common evaluation metrics with code</p>
    </d-title>

    <div id="contents" class="base-grid" style="border-top: 1px solid #eee; padding: 1.5rem 0;">
        <nav class="l-text toc figcaption">
            <h3>Contents</h3>
            <ul>
                <li>Accuracy
                    <ul>
                        <li>Binary classification</li>
                        <li>Multiclass classification</li>
                        <li>Multilabel binary classification</li>
                    </ul>
                </li>
                <li>Precision
                    <ul>
                        <li>Binary classification</li>
                        <li>Multiclass classification</li>
                        <li>Multilabel binary classification</li>
                    </ul>
                </li>
                <li>Recall
                    <ul>
                        <li>Binary classification</li>
                        <li>Multiclass classification</li>
                        <li>Multilabel binary classification</li>
                    </ul>
                </li>
                <li>f1
                    <ul>
                        <li>Binary classification</li>
                        <li>Multiclass classification</li>
                        <li>Multilabel binary classification</li>
                    </ul>
                </li>
                <li>AUC
                    <ul>
                        <li>Single-label binary classification</li>
                        <li>Multilabel binary classification</li>
                    </ul>
                </li>
                <li>Confusion matrix</li>
            </ul>
        </nav>
    </div>

    <d-article>

        <h2 id="bin-acc">Accuracy</h2>
        <h3 id="binary-classification">Binary classification</h3>
        <p>$$
            y = \left[\begin{array}{l}{0} \\ {1} \\ {0} \\ {1}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#228B22}{0} \\
            \textcolor{#228B22}{1} \\ \textcolor{#228B22}{0} \\ {0}\end{array}\right]
            $$</p>
        <p>$$ \text{accuracy} = \frac{\textcolor{#228B22}{\text{no. of matches}}}{\#total} = \frac{3}{4} $$</p>
        <d-code block language="python">
            from sklearn.metrics import accuracy_score
            y_true = [0, 1, 0, 1]
            y_pred = [0, 1, 0, 0]
            accuracy_score(y_true, y_pred)
        </d-code>
        <h3 id="mul-acc">Multiclass classification</h3>
        <p>$$
                y = \left[\begin{array}{l}{0} \\ {2} \\ {1} \\ {3}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#228B22}{0} \\
                {1} \\ {2} \\ \textcolor{#228B22}{3}\end{array}\right]
                $$</p>
        <p>$$ accuracy = \frac{\textcolor{#228B22}{\#same}}{\#total} = \frac{2}{4} $$</p>
        <d-code block language="python">
            from sklearn.metrics import accuracy_score
            y_true = [0, 2, 1, 3]
            y_pred = [0, 1, 2, 3]
            accuracy_score(y_true, y_pred)
        </d-code>
        <h3 id="mlb-acc">Multilabel binary classification</h3>
        <p>$$
            y = \left[\begin{array}{l}{[0,1,1,0]} \\ {[1,1,0,1]}\\ {[1,0,1,0]}\end{array}\right], \hat{y} =
            \left[\begin{array}{l}{[0,1,1,1]} \\ \textcolor{#228B22}{[1,1,0,1]}\\ {[0,1,0,1]}\end{array}\right]
            $$</p>
        <p>$$ accuracy = \frac{\textcolor{#228B22}{\#\text{exact matches}}}{\text{\#samples}} = \frac{1}{3} $$</p>
        <d-code block language="python">
            import numpy as np
            from sklearn.metrics import accuracy_score
            y_true = np.array([[0,1,1,0], [1,1,0,1], [1,0,1,0]])
            y_pred = np.array([[0,1,1,1], [1,1,0,1], [0,1,0,1]])
            accuracy_score(y_true, y_pred)
        </d-code>



        <h2 id="bin-pre">Precision</h2>
        <p>Ability of classifier to label positives correctly</p>
        <h3 id="bin-pre">Binary classification</h3>
        <p>$$
            y = \left[\begin{array}{l}\textcolor{#D8D8D8}{0} \\ {1} \\ {1} \\ \textcolor{#D8D8D8}{0} \\ {0}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#D8D8D8}{0} \\
            \textcolor{#228B22}{1} \\ \textcolor{#228B22}{1} \\ \textcolor{#D8D8D8}{0} \\ \textcolor{#FF0000}{1}\end{array}\right]
            $$</p>
        <p>$$ precision = \frac{\textcolor{#228B22}{tp}}{\textcolor{#228B22}{tp} + \textcolor{#FF0000}{fp}} = \frac{2}{2+1} $$</p>
        <d-code block language="python">
            from sklearn.metrics import precision_score
            y_true = [0, 1, 1, 0, 0]
            y_pred = [0, 1, 1, 0, 1]
            precision_score(y_true, y_pred)
        </d-code>
        <h3 id="multiclass-classification">Multiclass classification</h3>
        <h4 id="mul-pre-micro">Micro precision<d-footnote>Identical to accuracy in multiclass setting.</d-footnote></h4>
        <p>$$
            y = \left[\begin{array}{l}{0} \\ {1} \\ {2} \\ {0}\\ {1}\\ {2}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#228B22}{0} \\ {2} \\ {1} \\ \textcolor{#228B22}{0}\\ {0}\\ {1}\end{array}\right]
            $$</p>
        <p>$$ precision_\text{micro} = \frac{\textcolor{#228B22}{\#same}}{\#total} = accuracy = \frac{2}{6} $$</p>
        <d-code block language="python">
            from sklearn.metrics import precision_score
            y_true = [0, 1, 2, 0, 1, 2]
            y_pred = [0, 2, 1, 0, 0, 1]
            precision_score(y_true, y_pred, average='micro')
        </d-code>
        <h4 id="mul-pre-macro">Macro precision</h4>
        <p>$$
            y = \left[\begin{array}{l}{0} \\ {2} \\ {2} \\ {0}\\ {1}\\ {2}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#3b70c8}{0} \\ \textcolor{#FFA500}{2} \\ \textcolor{#975f8a}{1} \\ \textcolor{#3b70c8}{0}\\ \textcolor{#3b70c8}{0}\\ \textcolor{#975f8a}{1}\end{array}\right]
            $$</p>
        <p>$$ precision_\text{macro} = \frac{1}{|L|} \sum_{l \in L} \frac{|y_l \cap \hat{y}_l|}{|\hat{y}_l|} = \frac{1}{3} (\textcolor{#3b70c8}{\frac{2}{3}}+\textcolor{#975f8a}{\frac{0}{2}}+\textcolor{#FFA500}{\frac{1}{1}}) = \frac{5}{9} $$</p>
        <d-code block language="python">
            from sklearn.metrics import precision_score
            y_true = [0, 2, 2, 0, 1, 2]
            y_pred = [0, 2, 1, 0, 0, 1]
            precision_score(y_true, y_pred, average='macro')
        </d-code>
        <h3 id="multiclass-classification">Multilabel binary classification</h3>
        <h4 id="mlb-pre-micro">Micro precision</h4>
        <p>$$
                y = \left[\begin{array}{l}{[0,1,1,0]} \\ {[1,1,0,1]}\\ {[1,0,1,0]}\end{array}\right], \hat{y} =
                \left[\begin{array}{l}{[0,\textcolor{#228B22}{1},\textcolor{#228B22}{1},\textcolor{#FF0000}{1}]} \\ [\textcolor{#228B22}{1},\textcolor{#228B22}{1},0,\textcolor{#228B22}{1}]\\ {[0,\textcolor{#FF0000}{1},0,\textcolor{#FF0000}{1}]}\end{array}\right]
                $$</p>
        <p>$$ precision_\text{micro} = \frac{\textcolor{#228B22}{\text{no. of 1's in }\hat{y} \text{ that match those in } y}}{\text{no. of 1's in } \hat{y}} = \frac{5}{8} $$</p>
        <d-code block language="python">
            from sklearn.metrics import precision_score
            y_true = np.array([[0,1,1,0], [1,1,0,1], [1,0,1,0]])
            y_pred = np.array([[0,1,1,1], [1,1,0,1], [0,1,0,1]])
            precision_score(y_true, y_pred, average='micro')
        </d-code>
        <h4 id="mlb-pre-macro">Macro precision</h4>
        <p>$$
                y = \left[\begin{array}{l}{[0,1,1,0]} \\ {[1,1,0,1]}\\ {[1,0,1,0]}\end{array}\right], \hat{y} =
                \left[\begin{array}{l}{[0,1,1,1]} \\ \textcolor{#228B22}{[1,1,0,1]}\\ {[0,1,0,1]}\end{array}\right]
                $$</p>
        <p>Note: There are 4 labels in each sample. Calculate the precision for one label at a time, then take the average of all the 4 labels.</p>
        <p>$$ precision_\text{macro} = \frac{1}{|L|} \sum_{l \in L} \frac{|y_l \cap \hat{y}_l|}{|\hat{y}_l|} = \frac{1}{4}(\frac{1}{1}+\frac{2}{3}+\frac{1}{1}+\frac{1}{3}) = \frac{3}{4} $$</p>
        <d-code block language="python">
            from sklearn.metrics import precision_score
            y_true = np.array([[0,1,1,0], [1,1,0,1], [1,0,1,0]])
            y_pred = np.array([[0,1,1,1], [1,1,0,1], [0,1,0,1]])
            precision_score(y_true, y_pred, average='macro')
        </d-code>



        <h2 id="bin-rec">Recall</h2>
        <p>Ability of classifier to recover the positives</p>
        <h3 id="bin-pre">Binary classification</h3>
        <p>$$
            y = \left[\begin{array}{l}\textcolor{#D8D8D8}{0} \\ {1} \\ \textcolor{#D8D8D8}{0} \\ {1} \\ {1}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#D8D8D8}{0} \\
            \textcolor{#228B22}{1} \\ \textcolor{#D8D8D8}{0} \\ \textcolor{#FF0000}{0} \\ \textcolor{#FF0000}{0}\end{array}\right]
            $$</p>
        <p>$$ recall = \frac{\textcolor{#228B22}{tp}}{\textcolor{#228B22}{tp} + \textcolor{#FF0000}{fn}} = \frac{1}{1+2} $$</p>
        <d-code block language="python">
            from sklearn.metrics import recall_score
            y_true = [0, 1, 0, 1, 1]
            y_pred = [0, 1, 0, 0, 0]
            recall_score(y_true, y_pred)
        </d-code>
        <h3>Multiclass classification</h3>
        <h4 id="mul-rec-micro">Micro recall<d-footnote>Identical to accuracy in multiclass setting.</d-footnote></h4>
        <p>$$
            y = \left[\begin{array}{l}{0} \\ {1} \\ {2} \\ {0}\\ {1}\\ {2}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#228B22}{0} \\ {2} \\ {1} \\ \textcolor{#228B22}{0}\\ {0}\\ {1}\end{array}\right]
            $$</p>
        <p>$$ precision_\text{micro} = \frac{\textcolor{#228B22}{\#same}}{\#total} = accuracy = \frac{2}{6} $$</p>
        <d-code block language="python">
            from sklearn.metrics import recall_score
            y_true = [0, 1, 2, 0, 1, 2]
            y_pred = [0, 2, 1, 0, 0, 1]
            recall_score(y_true, y_pred, average='micro')
        </d-code>
        <h4 id="mul-rec-macro">Macro recall</h4>
        <p>$$
                y \left[\begin{array}{l}\textcolor{#3b70c8}{0} \\ \textcolor{#FFA500}{2} \\ \textcolor{#FFA500}{2} \\ \textcolor{#3b70c8}{0}\\ \textcolor{#975f8a}{1}\\ \textcolor{#FFA500}{2}\end{array}\right]
                  \left[\begin{array}{l}{0} \\ {2} \\ {1} \\ {0}\\ {0}\\ {1}\end{array}\right] \hat{y}
                $$</p>
        <p>$$ precision_\text{macro} = \frac{1}{|L|} \sum_{l \in L} \frac{|y_l \cap \hat{y}_l|}{|y_l|} = \frac{1}{3} (\textcolor{#3b70c8}{\frac{2}{2}}+\textcolor{#975f8a}{\frac{0}{1}}+\textcolor{#FFA500}{\frac{1}{3}}) = \frac{4}{9} $$</p>
        <d-code block language="python">
            from sklearn.metrics import recall_score
            y_true = [0, 2, 2, 0, 1, 2]
            y_pred = [0, 2, 1, 0, 0, 1]
            recall_score(y_true, y_pred, average='macro')
        </d-code>
        <h3>Multilabel binary classification</h3>
        <h4 id="mlb-rec-micro">Micro recall</h3>
        <p>$$
            y = \left[\begin{array}{l}{[0,\textcolor{#228B22}{1},\textcolor{#228B22}{1},0]} \\ [\textcolor{#228B22}{1},\textcolor{#228B22}{1},0,\textcolor{#228B22}{1}]\\ {[\textcolor{#FF0000}{1},0,\textcolor{#FF0000}{1},0]}\end{array}\right]
            ,   \left[\begin{array}{l}{[0,1,1,0]} \\ {[1,1,0,1]}\\ {[0,1,0,1]}\end{array}\right]
                $$</p>
        <p>$$ recall_\text{micro} = \frac{\textcolor{#228B22}{\text{no. of 1's in }y \text{ that match those in } \hat{y}}}{\text{no. of 1's in } y} = \frac{5}{7} $$</p>
        <d-code block language="python">
            from sklearn.metrics import recall_score
            y_true = np.array([[0,1,1,0], [1,1,0,1], [1,0,1,0]])
            y_pred = np.array([[0,1,1,1], [1,1,0,1], [0,1,0,1]])
            recall_score(y_true, y_pred, average='micro')
        </d-code>
        <h4 id="mlb-rec-macro">Macro recall</h4>
        <p>$$
                y = \left[\begin{array}{l}{[0,1,1,0]} \\ {[1,1,0,1]}\\ {[1,0,1,0]}\end{array}\right], \hat{y} =
                \left[\begin{array}{l}{[0,1,1,1]} \\ \textcolor{#228B22}{[1,1,0,1]}\\ {[0,1,0,1]}\end{array}\right]
                $$</p>
        <p>Note: There are 4 labels in each sample. Calculate the recall for one label at a time, then take the average of all the 4 labels.</p>
        <p>$$ precision_\text{macro} = \frac{1}{|L|} \sum_{l \in L} \frac{|y_l \cap \hat{y}_l|}{|y_l|} =  \frac{1}{4}(\frac{1}{2}+\frac{2}{2}+\frac{1}{2}+\frac{1}{1}) = \frac{3}{4} $$</p>
        <d-code block language="python">
            from sklearn.metrics import recall_score
            y_true = np.array([[0,1,1,0], [1,1,0,1], [1,0,1,0]])
            y_pred = np.array([[0,1,1,1], [1,1,0,1], [0,1,0,1]])
            recall_score(y_true, y_pred, average='macro')
        </d-code>



        <h2 id="bin-f1">F1</h2>
        <h3 id="bin-pre">Binary classification</h3>
        <p>$$ f1 = 2 \cdot \frac{precision \times recall}{ precision + recall} $$</p>
        <d-code block language="python">
            from sklearn.metrics import f1_score
            y_true = [0, 1, 0, 1, 1]
            y_pred = [0, 1, 0, 0, 0]
            f1_score(y_true, y_pred)
        </d-code>
        <h3 id="bin-pre">Multiclass classification</h3>
        <h4 id="mul-f1-micro">Micro F1<d-footnote>Identical to accuracy in multiclass setting.</d-footnote></h4>
        <p>$$ f1_{\text{micro}} = 2 \cdot \frac{precision_{\text{micro}} \times recall_{\text{micro}}}{ precision_{\text{micro}} + recall_{\text{micro}}} = accuracy$$</p>
        <d-code block language="python">
            from sklearn.metrics import f1_score
            y_true = [0, 2, 2, 0, 1, 2]
            y_pred = [0, 2, 1, 0, 0, 1]
            f1_score(y_true, y_pred, average='micro')
        </d-code>
        <h4 id="mul-f1-macro">Macro F1</h4>
        <p>$$ f1_{\text{macro}} = 2 \cdot \frac{precision_{\text{macro}} \times recall_{\text{macro}}}{ precision_{\text{macro}} + recall_{\text{macro}}} $$</p>
        <d-code block language="python">
            from sklearn.metrics import f1_score
            y_true = [0, 2, 2, 0, 1, 2]
            y_pred = [0, 2, 1, 0, 0, 1]
            f1_score(y_true, y_pred, average='macro')
        </d-code>
        <h3 id="multilabel-binary-classification">Multilabel binary classification</h3>
        <h4 id="mlb-f1-micro">Micro F1</h4>
        <p>$$ f1_{\text{micro}} = 2 \cdot \frac{precision_{\text{micro}} \times recall_{\text{micro}}}{ precision_{\text{micro}} + recall_{\text{micro}}}$$</p>
        <d-code block language="python">
            from sklearn.metrics import f1_score
            y_true = np.array([[0,1,1,0], [1,1,0,1], [1,0,1,0]])
            y_pred = np.array([[0,1,1,1], [1,1,0,1], [0,1,0,1]])
            f1_score(y_true, y_pred, average='micro')
        </d-code>

        <h4 id="mlb-f1-macro">Macro F1</h4>
        <p>$$ f1_{\text{macro}} = 2 \cdot \frac{precision_{\text{macro}} \times recall_{\text{macro}}}{ precision_{\text{macro}} + recall_{\text{macro}}} $$</p>
        <d-code block language="python">
            from sklearn.metrics import f1_score
            y_true = np.array([[0,1,1,0], [1,1,0,1], [1,0,1,0]])
            y_pred = np.array([[0,1,1,1], [1,1,0,1], [0,1,0,1]])
            f1_score(y_true, y_pred, average='macro')
        </d-code>



        <h2 id="bin-auc">AUC / AUROC (Area Under Curve / Area Under ROC)<d-footnote>Your predicted values must be scores.</d-footnote></h2>
        <p>ROC is plotted on a graph of TPR over FPR, and the area below this graph is called area
            under curve (AUC). AUC represents the probability that a random positive example is
            positioned to the right of a random negative example. See
            <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">here</a>.
        </p>
        <p>Suppose</p>
        <p>$$
            y = \left[\begin{array}{l}{0} \\ {0} \\ {1}\end{array}\right], \hat{y} = \left[\begin{array}{l}{0.8} \\
            {0.3} \\ {0.6}\end{array}\right]
            $$</p>

            <p>Use 4 different thresholds, above any of which will be considered positive, below of which negative.
                Here we chose [1.8, 0.8, 0.6, and 0.3].
            </p>

            <p>Threshold = 1.8</p>
            <p>$$
                    y = \left[\begin{array}{l}{0} \\ {0} \\ {1}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#76BED0}{0} \\
                        \textcolor{#76BED0}{0} \\ \textcolor{#F4A261}{0} \end{array}\right]
                    $$</p>
            <p>$$ fpr = \frac{\textcolor{#FF0000}{fp}}{\textcolor{#FF0000}{fp} + \textcolor{#76BED0}{tn}} = \frac{0}{0+2} = 0 $$</p>
            <p>$$ tpr = \frac{\textcolor{#228B22}{tp}}{\textcolor{#228B22}{tp} + \textcolor{#F4A261}{fn}} = \frac{0}{0+1} = 0 $$</p>
            <p>Threshold = 0.8</p>
            <p>$$
                y = \left[\begin{array}{l}{0} \\ {0} \\ {1}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#FF0000}{1} \\
                \textcolor{#76BED0}{0} \\ \textcolor{#F4A261}{0}\end{array}\right]
                $$</p>
                <p>$$ fpr = \frac{\textcolor{#FF0000}{fp}}{\textcolor{#FF0000}{fp} + \textcolor{#76BED0}{tn}} = \frac{1}{1+1} = \frac{1}{2} $$</p>
            <p>$$ tpr = \frac{\textcolor{#228B22}{tp}}{\textcolor{#228B22}{tp} + \textcolor{#F4A261}{fn}} = \frac{0}{0+1} = 0 $$</p>
                <p>Threshold = 0.6</p>
                <p>$$
                    y = \left[\begin{array}{l}{0} \\ {0} \\ {1}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#FF0000}{1} \\
                    \textcolor{#76BED0}{0} \\ \textcolor{#228B22}{1}\end{array}\right]
                    $$</p>
                    <p>$$ fpr = \frac{\textcolor{#FF0000}{fp}}{\textcolor{#FF0000}{fp} + \textcolor{#76BED0}{tn}} = \frac{1}{1+1} = \frac{1}{2} $$</p>
                    <p>$$ tpr = \frac{\textcolor{#228B22}{tp}}{\textcolor{#228B22}{tp} + \textcolor{#F4A261}{fn}} = \frac{1}{1+0} = 1 $$</p>

                    <p>Threshold = 0.3</p>
                <p>$$
                    y = \left[\begin{array}{l}{0} \\ {0} \\ {1}\end{array}\right], \hat{y} = \left[\begin{array}{l}\textcolor{#FF0000}{1} \\
                    \textcolor{#FF0000}{1} \\ \textcolor{#228B22}{1}\end{array}\right]
                    $$</p>
                    <p>$$ fpr = \frac{\textcolor{#FF0000}{fp}}{\textcolor{#FF0000}{fp} + \textcolor{#76BED0}{tn}} = \frac{2}{2+0} = 1 $$</p>
                    <p>$$ tpr = \frac{\textcolor{#228B22}{tp}}{\textcolor{#228B22}{tp} + \textcolor{#F4A261}{fn}} = \frac{1}{1+0} = 1 $$</p>

                    <p>And so you have a list of fpr's and tpr's.</p>

            <d-code block language="python">
            from sklearn.metrics import roc_curve
            y_true = [  0,   0,   1]
            y_pred = [0.8, 0.3, 0.6]
            fpr, tpr, thresholds = (y_true, y_pred)
            </d-code>
        <p>For multilabel binary classification</p>
        <d-code block language="python">
            from sklearn.metrics import roc_auc_score
            y_true = np.array([[0,1,1,0], [1,1,0,1], [1,0,1,0]])
            y_pred = np.array([[0,1,1,1], [1,1,0,1], [0,1,0,1]])
            roc_auc_score(y_true, y_pred)
        </d-code>



        <h2 id="confusion-matrix">Confusion matrix</h2>
        <img src="B3459ABE-0EF5-40D3-BF70-62B884269DE7 copy.jpg" width="250">
        <d-code block language="python">
            >>> from sklearn.metrics import confusion_matrix
            >>> y_true = [2, 0, 2, 2, 0, 1]
            >>> y_pred = [0, 0, 2, 2, 0, 2]
            >>> confusion_matrix(y_true, y_pred)
            array([[2, 0, 0],
                   [0, 0, 1],
                   [1, 0, 2]])
        </d-code>



        <h2 id="multilabel-multiclass-classification">Notes</h2>
        For Multilabel multiclass classification, Redefine your model and targets to use the metrics above (?)

    </d-article>

    <d-appendix>
        <d-footnote-list></d-footnote-list>
    </d-appendix>

    <script type="text/bibliography">
            @article{gregor2015draw,
              title={DRAW: A recurrent neural network for image generation},
              author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
              journal={arXivreprint arXiv:1502.04623},
              year={2015},
              url={https://arxiv.org/pdf/1502.04623.pdf}
            }
    </script>

</body>
</html>